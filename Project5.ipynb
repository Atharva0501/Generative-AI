{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAJszstc2ngn",
        "outputId": "87a3b02e-b5be-45ff-f692-39ec1df41f42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 18.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 314kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 4.99MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 14.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load FashionMNIST Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install minisom\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8Sq7oQu3JJF",
        "outputId": "539c2002-acfa-46a0-acbd-52169ca73976"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minisom\n",
            "  Downloading MiniSom-2.3.3.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: minisom\n",
            "  Building wheel for minisom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minisom: filename=MiniSom-2.3.3-py3-none-any.whl size=11706 sha256=b5c6c02d0889324ef671bc8b2226516d0d6c765a04b4502a623e986149b461b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/98/a5/52dee3e8ed1dbfc4d77e4da41b6d89dd7ab9ead1b921e766f8\n",
            "Successfully built minisom\n",
            "Installing collected packages: minisom\n",
            "Successfully installed minisom-2.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from minisom import MiniSom\n",
        "import numpy as np\n",
        "\n",
        "# Initialize SOM\n",
        "som = MiniSom(x=10, y=10, input_len=784, sigma=1.0, learning_rate=0.5)\n",
        "\n",
        "# Train SOM with flattened images\n",
        "train_data = train_dataset.data.view(-1, 784).numpy()\n",
        "som.train_random(data=train_data, num_iteration=100)\n",
        "\n",
        "# Get SOM representations\n",
        "som_transformed = np.array([som.winner(x) for x in train_data])\n"
      ],
      "metadata": {
        "id": "0d9L66ky3oVk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, visible_dim, hidden_dim):\n",
        "        super(RBM, self).__init__()\n",
        "        self.W = nn.Parameter(torch.randn(hidden_dim, visible_dim) * 0.1)\n",
        "        self.h_bias = nn.Parameter(torch.zeros(hidden_dim))\n",
        "        self.v_bias = nn.Parameter(torch.zeros(visible_dim))\n",
        "\n",
        "    def forward(self, v):\n",
        "        h_prob = torch.sigmoid(F.linear(v, self.W, self.h_bias))\n",
        "        h_sample = torch.bernoulli(h_prob)\n",
        "        return h_sample\n",
        "\n",
        "rbm = RBM(784, 128)\n",
        "# Get RBM representations\n",
        "train_data_tensor = torch.tensor(train_data).float()\n",
        "rbm_output = rbm(train_data_tensor).detach().numpy()\n"
      ],
      "metadata": {
        "id": "Mqyc13AK36Uc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "vae = VAE(784, 256, 64)\n",
        "\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    # Mean Squared Error Loss\n",
        "    MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    # KL Divergence Loss remains the same\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return MSE + KLD\n",
        "\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "for epoch in range(10):\n",
        "    for data, _ in train_loader:\n",
        "        data = data.view(-1, 784)\n",
        "        recon_batch, mu, logvar = vae(data)\n",
        "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Get VAE representations\n",
        "vae_output = vae(train_data_tensor)[0].detach().numpy()\n"
      ],
      "metadata": {
        "id": "8OuMSxZc4ASs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def evaluate_model(X_train, y_train, X_test, y_test):\n",
        "\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_train = imputer.fit_transform(X_train)\n",
        "    X_test = imputer.transform(X_test)\n",
        "\n",
        "    # Initialize classifiers\n",
        "    rf = RandomForestClassifier()\n",
        "    ab = AdaBoostClassifier(algorithm='SAMME')\n",
        "\n",
        "    # Train and evaluate RandomForest\n",
        "    rf.fit(X_train, y_train)\n",
        "    rf_acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "\n",
        "    # Train and evaluate AdaBoost\n",
        "    ab.fit(X_train, y_train)\n",
        "    ab_acc = accuracy_score(y_test, ab.predict(X_test))\n",
        "\n",
        "    return rf_acc, ab_acc\n",
        "\n",
        "# Original Data\n",
        "rf_acc_orig, ab_acc_orig = evaluate_model(train_data, y_train, test_data, y_test)\n",
        "\n",
        "# SOM Data\n",
        "rf_acc_som, ab_acc_som = evaluate_model(som_transformed, y_train, som_test_transformed, y_test)\n",
        "\n",
        "# RBM Data\n",
        "rf_acc_rbm, ab_acc_rbm = evaluate_model(rbm_output, y_train, rbm_test_output, y_test)\n",
        "\n",
        "# VAE Data\n",
        "rf_acc_vae, ab_acc_vae = evaluate_model(vae_output, y_train, vae_test_output, y_test)\n",
        "\n",
        "# Print results\n",
        "print(f\"Original RF: {rf_acc_orig:.4f}, AdaBoost: {ab_acc_orig:.4f}\")\n",
        "print(f\"SOM RF: {rf_acc_som:.4f}, AdaBoost: {ab_acc_som:.4f}\")\n",
        "print(f\"RBM RF: {rf_acc_rbm:.4f}, AdaBoost: {ab_acc_rbm:.4f}\")\n",
        "print(f\"VAE RF: {rf_acc_vae:.4f}, AdaBoost: {ab_acc_vae:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOMyF-iF_3fK",
        "outputId": "9b6cfec6-974e-48da-deb0-dd46bb4f23d6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original RF: 0.8770, AdaBoost: 0.5089\n",
            "SOM RF: 0.5467, AdaBoost: 0.3782\n",
            "RBM RF: 0.8070, AdaBoost: 0.4637\n",
            "VAE RF: 0.3234, AdaBoost: 0.2735\n"
          ]
        }
      ]
    }
  ]
}