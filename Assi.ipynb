{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a neural network with at least two hidden layers for a classification task. The dataset should be CIFAR10.\n",
    "Experiment with three activation functions (one linear and one non-linear) and report (i) accuracy and (ii) execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/atharvasharma/miniconda3/envs/myenv/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /Users/atharvasharma/miniconda3/envs/myenv/lib/python3.8/site-packages (0.19.1)\n",
      "Requirement already satisfied: numpy in /Users/atharvasharma/miniconda3/envs/myenv/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already satisfied: filelock in /Users/atharvasharma/miniconda3/envs/myenv/lib/python3.8/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/atharvasharma/miniconda3/envs/myenv/lib/python3.8/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/atharvasharma/miniconda3/envs/myenv/lib/python3.8/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/atharvasharma/miniconda3/envs/myenv/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/atharvasharma/miniconda3/envs/myenv/lib/python3.8/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/atharvasharma/miniconda3/envs/myenv/lib/python3.8/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/atharvasharma/miniconda3/envs/myenv/lib/python3.8/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/atharvasharma/miniconda3/envs/myenv/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/atharvasharma/miniconda3/envs/myenv/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Experimenting with ReLU activation function...\n",
      "Epoch [1/10], Loss: 1.6442\n",
      "Epoch [2/10], Loss: 1.4537\n",
      "Epoch [3/10], Loss: 1.3751\n",
      "Epoch [4/10], Loss: 1.3142\n",
      "Epoch [5/10], Loss: 1.2602\n",
      "Epoch [6/10], Loss: 1.2177\n",
      "Epoch [7/10], Loss: 1.1785\n",
      "Epoch [8/10], Loss: 1.1426\n",
      "Epoch [9/10], Loss: 1.1120\n",
      "Epoch [10/10], Loss: 1.0774\n",
      "ReLU - Accuracy: 51.59%, Execution Time: 28.48s\n",
      "\n",
      "Experimenting with Tanh activation function...\n",
      "Epoch [1/10], Loss: 1.7578\n",
      "Epoch [2/10], Loss: 1.6407\n",
      "Epoch [3/10], Loss: 1.5859\n",
      "Epoch [4/10], Loss: 1.5459\n",
      "Epoch [5/10], Loss: 1.5131\n",
      "Epoch [6/10], Loss: 1.4881\n",
      "Epoch [7/10], Loss: 1.4630\n",
      "Epoch [8/10], Loss: 1.4455\n",
      "Epoch [9/10], Loss: 1.4204\n",
      "Epoch [10/10], Loss: 1.3971\n",
      "Tanh - Accuracy: 45.70%, Execution Time: 28.16s\n",
      "\n",
      "Experimenting with Identity (Linear) activation function...\n",
      "Epoch [1/10], Loss: 1.8398\n",
      "Epoch [2/10], Loss: 1.7581\n",
      "Epoch [3/10], Loss: 1.7407\n",
      "Epoch [4/10], Loss: 1.7266\n",
      "Epoch [5/10], Loss: 1.7200\n",
      "Epoch [6/10], Loss: 1.7109\n",
      "Epoch [7/10], Loss: 1.7073\n",
      "Epoch [8/10], Loss: 1.6999\n",
      "Epoch [9/10], Loss: 1.6950\n",
      "Epoch [10/10], Loss: 1.6916\n",
      "Identity (Linear) - Accuracy: 39.52%, Execution Time: 27.96s\n",
      "\n",
      "Final Results:\n",
      "ReLU: Accuracy: 51.59%, Execution Time: 28.48s\n",
      "Tanh: Accuracy: 45.70%, Execution Time: 28.16s\n",
      "Identity (Linear): Accuracy: 39.52%, Execution Time: 27.96s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformation (No resize, as CIFAR-10 is 32x32)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalization for CIFAR-10\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Neural Network class with flexible activation function\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, activation_function):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3072, 128)  # Adjusted input size to 3072 (32x32x3)\n",
    "        self.fc2 = nn.Linear(128, 10)   # Hidden to output layer\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3072)  # Flatten the input to 3072\n",
    "        x = self.activation_function(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader):.4f}')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Test function to calculate accuracy\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Experiment with activation functions\n",
    "activation_functions = {\n",
    "    'ReLU': nn.ReLU(),\n",
    "    'Tanh': nn.Tanh(),\n",
    "    'Identity (Linear)': nn.Identity()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, activation_function in activation_functions.items():\n",
    "    print(f\"\\nExperimenting with {name} activation function...\")\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = SimpleNN(activation_function).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model and calculate execution time\n",
    "    exec_time = train_model(model, criterion, optimizer, num_epochs=10)\n",
    "    \n",
    "    # Test the model and calculate accuracy\n",
    "    accuracy = test_model(model)\n",
    "    \n",
    "    # Store the results\n",
    "    results[name] = (accuracy, exec_time)\n",
    "    print(f\"{name} - Accuracy: {accuracy:.2f}%, Execution Time: {exec_time:.2f}s\")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nFinal Results:\")\n",
    "for name, (accuracy, exec_time) in results.items():\n",
    "    print(f\"{name}: Accuracy: {accuracy:.2f}%, Execution Time: {exec_time:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with three optimizers and report (i) accuracy and (ii) execution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Experimenting with SGD optimizer...\n",
      "Experimenting with Adam optimizer...\n",
      "Experimenting with RMSprop optimizer...\n",
      "SGD - Accuracy: 52.14%, Execution Time: 37.92s\n",
      "Adam - Accuracy: 52.37%, Execution Time: 47.70s\n",
      "RMSprop - Accuracy: 51.58%, Execution Time: 43.50s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a simple neural network with the correct input size\n",
    "class ExperimentNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExperimentNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3072, 512)  # Adjust input size to 3072 (32x32x3)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3072)  # Flatten the input to 3072\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Function to train and evaluate the model with different optimizers\n",
    "def experiment(optimizer_name, optimizer_fn):\n",
    "    model = ExperimentNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer_fn(model.parameters())\n",
    "\n",
    "    # Training the model\n",
    "    start_time = time.time()\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Evaluate the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, execution_time\n",
    "\n",
    "# Experimenting with three optimizers\n",
    "optimizers = {\n",
    "    \"SGD\": lambda params: optim.SGD(params, lr=0.001, momentum=0.9),\n",
    "    \"Adam\": lambda params: optim.Adam(params, lr=0.001),\n",
    "    \"RMSprop\": lambda params: optim.RMSprop(params, lr=0.001)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, optimizer_fn in optimizers.items():\n",
    "    print(f\"Experimenting with {name} optimizer...\")\n",
    "    accuracy, execution_time = experiment(name, optimizer_fn)\n",
    "    results[name] = {\"Accuracy\": accuracy, \"Execution Time (s)\": execution_time}\n",
    "\n",
    "# Display the results\n",
    "for name, result in results.items():\n",
    "    print(f\"{name} - Accuracy: {result['Accuracy']:.2f}%, Execution Time: {result['Execution Time (s)']:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with “Dropout layer”, “BatchNorm” and “Weight initialization” and\n",
    "report changes in accuracy and execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Experimenting with Baseline...\n",
      "Experimenting with Dropout...\n",
      "Experimenting with BatchNorm...\n",
      "Experimenting with Weight Initialization...\n",
      "Experimenting with Dropout + BatchNorm + Weight Init...\n",
      "Baseline - Accuracy: 49.46%, Execution Time: 156.44s\n",
      "Dropout - Accuracy: 40.25%, Execution Time: 157.14s\n",
      "BatchNorm - Accuracy: 51.09%, Execution Time: 158.06s\n",
      "Weight Initialization - Accuracy: 47.28%, Execution Time: 156.45s\n",
      "Dropout + BatchNorm + Weight Init - Accuracy: 46.94%, Execution Time: 160.24s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a neural network with options for Dropout, BatchNorm, and Weight Initialization\n",
    "class ExperimentNN(nn.Module):\n",
    "    def __init__(self, use_dropout=False, use_batchnorm=False, use_weight_init=False):\n",
    "        super(ExperimentNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3072, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.use_dropout = use_dropout\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        if self.use_batchnorm:\n",
    "            self.bn1 = nn.BatchNorm1d(512)\n",
    "            self.bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        if use_weight_init:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3072)  # Flatten the input\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        if self.use_batchnorm:\n",
    "            x = self.bn1(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = torch.relu(self.fc2(x))\n",
    "        if self.use_batchnorm:\n",
    "            x = self.bn2(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "# Function to train and evaluate the model with different configurations\n",
    "def experiment(config_name, use_dropout=False, use_batchnorm=False, use_weight_init=False):\n",
    "    model = ExperimentNN(use_dropout=use_dropout, use_batchnorm=use_batchnorm, use_weight_init=use_weight_init)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training the model\n",
    "    start_time = time.time()\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Evaluate the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, execution_time\n",
    "\n",
    "# Experimenting with different settings\n",
    "configs = {\n",
    "    \"Baseline\": {\"use_dropout\": False, \"use_batchnorm\": False, \"use_weight_init\": False},\n",
    "    \"Dropout\": {\"use_dropout\": True, \"use_batchnorm\": False, \"use_weight_init\": False},\n",
    "    \"BatchNorm\": {\"use_dropout\": False, \"use_batchnorm\": True, \"use_weight_init\": False},\n",
    "    \"Weight Initialization\": {\"use_dropout\": False, \"use_batchnorm\": False, \"use_weight_init\": True},\n",
    "    \"Dropout + BatchNorm + Weight Init\": {\"use_dropout\": True, \"use_batchnorm\": True, \"use_weight_init\": True},\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for config_name, config in configs.items():\n",
    "    print(f\"Experimenting with {config_name}...\")\n",
    "    accuracy, execution_time = experiment(\n",
    "        config_name, \n",
    "        use_dropout=config[\"use_dropout\"], \n",
    "        use_batchnorm=config[\"use_batchnorm\"], \n",
    "        use_weight_init=config[\"use_weight_init\"]\n",
    "    )\n",
    "    results[config_name] = {\"Accuracy\": accuracy, \"Execution Time (s)\": execution_time}\n",
    "\n",
    "# Display the results\n",
    "for name, result in results.items():\n",
    "    print(f\"{name} - Accuracy: {result['Accuracy']:.2f}%, Execution Time: {result['Execution Time (s)']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
